{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Naive Bayes is a class of simple classifiers based on Bayes' Rule and strong (or naive) independence assumptions between features. In this problem, you will implement a Naive Bayes Classifier for the Census Income Data Set from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/).\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The dataset consists 32561 instances, each representing an individual. The goal is to predict whether a person makes over 50K a year based on 14 features. The features are:\n",
    "\n",
    "| column | type | description |\n",
    "| --- |:---:|:--- |\n",
    "| age | continuous | trips around the sun to date\n",
    "| final_weight | continuous | census weight attribute; constructed from the original census data |\n",
    "| education_num | continuous | numeric education scale -- their maximum educational level as a number |\n",
    "| capital_gain | continuous | income from investment sources |\n",
    "| capital_loss | continuous | losses from investment sources |\n",
    "| hours_per_week | continuous | number of hours worked every week |\n",
    "| work_class | categorical | `Private`, `Self-emp-not-inc`, `Self-emp-inc`, `Federal-gov`, `Local-gov`, `State-gov`, `Without-pay`, `Never-worked` |\n",
    "| education | categorical | `Bachelors`, `Some-college`, `11th`, `HS-grad`, `Prof-school`, `Assoc-acdm`, `Assoc-voc`, `9th`, `7th-8th`, `12th`, `Masters`, `1st-4th`, `10th`, `Doctorate`, `5th-6th`, `Preschool` |\n",
    "| marital_status | categorical | `Married-civ-spouse`, `Divorced`, `Never-married`, `Separated`, `Widowed`, `Married-spouse-absent`, `Married-AF-spouse` |\n",
    "| occupation | categorical | `Tech-support`, `Craft-repair`, `Other-service`, `Sales`, `Exec-managerial`, `Prof-specialty`, `Handlers-cleaners`, `Machine-op-inspct`, `Adm-clerical`, `Farming-fishing`, `Transport-moving`, `Priv-house-serv`, `Protective-serv`, `Armed-Forces` |\n",
    "| relationship | categorical | `Wife`, `Own-child`, `Husband`, `Not-in-family`, `Other-relative`, `Unmarried.` |\n",
    "| race | categorical | `White`, `Asian-Pac-Islander`, `Amer-Indian-Eskimo`, `Other`, `Black` |\n",
    "| sex | categorical | `Female`, `Male` |\n",
    "| native_country | categorical | (41 values not shown here) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "import gzip\n",
    "from testing.testing import test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Data Preparation\n",
    "\n",
    "First, you need to load in the above data, provided to you as a CSV file. As the data is from UCI repository, it is already quite clean. However, some instances contain missing `occupation`, `native_country` or `work_class` (represented as ? in the CSV file) and these have to be discarded from the training set. Also, replace the `income` column with `label`, which is 1 if `income` is `>50K` and 0 otherwise. Finally, ensure you reset the index so the row numbers are contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(fn):\n",
    "    with gzip.open(fn, \"rt\", newline='', encoding=\"UTF-8\") as file:\n",
    "        return pd.read_csv(file)\n",
    "\n",
    "def load_data_test(load_data):\n",
    "    df = load_data()\n",
    "    \n",
    "    DF_TYPES = {\n",
    "        \"age\"  : \"int64\",\n",
    "        \"work_class\"  : \"object\",\n",
    "        \"final_weight\"  : \"int64\",\n",
    "        \"education\"  : \"object\",\n",
    "        \"education_num\"  : \"int64\",\n",
    "        \"marital_status\"  : \"object\",\n",
    "        \"occupation\"  : \"object\",\n",
    "        \"relationship\"  : \"object\",\n",
    "        \"race\"  : \"object\",\n",
    "        \"sex\"  : \"object\",\n",
    "        \"capital_gain\"  : \"int64\",\n",
    "        \"capital_loss\"  : \"int64\",\n",
    "        \"hours_per_week\"  : \"int64\",\n",
    "        \"native_country\"  : \"object\",\n",
    "        \"label\"  : \"int64\"\n",
    "    }\n",
    "\n",
    "    test.equal(DF_TYPES, { k: str(df[k].dtypes) for k in DF_TYPES })\n",
    "\n",
    "    # Check for blank entries:\n",
    "    test.equal(any(df['occupation'].eq(\"?\")), False)\n",
    "    test.equal(any(df['native_country'].eq(\"?\")), False)\n",
    "    test.equal(any(df['work_class'].eq(\"?\")), False)\n",
    "    \n",
    "    # Make sure there's no income column:\n",
    "    test.true(\"income\" not in df.columns)\n",
    "\n",
    "    # Index handling:\n",
    "    test.equal(repr(df.index), \"RangeIndex(start=0, stop=30162, step=1)\")\n",
    "    \n",
    "@test\n",
    "def load_data(file_name=\"census.csv.gz\"):\n",
    "    \"\"\" loads and processes data in the manner specified above\n",
    "\n",
    "    args:\n",
    "        file_name : str -- path to csv file containing data\n",
    "\n",
    "    returns: pd.DataFrame -- processed dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Naive Bayes classifier\n",
    "\n",
    "Let $X_1, X_2, \\ldots, X_k$ be the $k$ features of a dataset, with class label given by the variable $y$. A probabilistic classifier assigns the most probable class to each instance $(x_1,\\ldots,x_k)$, as expressed by\n",
    "$$ \\hat{y} = \\arg\\max_y P(y\\ \\mid\\ x_1,\\ldots,x_k) $$\n",
    "\n",
    "Using Bayes' theorem, the above *posterior probability* can be rewritten as\n",
    "$$ P(y\\ \\mid\\ x_1,\\ldots,x_k) = \\frac{P(y) P(x_1,\\ldots,x_n\\ \\mid\\ y)}{P(x_1,\\ldots,x_k)} $$\n",
    "where\n",
    "- $P(y)$ is the prior probability of the class\n",
    "- $P(x_1,\\ldots,x_k\\ \\mid\\ y)$ is the likelihood of data under a class\n",
    "- $P(x_1,\\ldots,x_k)$ is the evidence for data\n",
    "\n",
    "Naive Bayes classifiers assume that the feature values are conditionally independent given the class label, that is,\n",
    "$ P(x_1,\\ldots,x_n\\ \\mid\\ y) = \\prod_{i=1}^{k}P(x_i\\ \\mid\\ y) $. This strong assumption helps simplify the expression for posterior probability to\n",
    "$$ P(y\\ \\mid\\ x_1,\\ldots,x_k) = \\frac{P(y) \\prod_{i=1}^{k}P(x_i\\ \\mid\\ y)}{P(x_1,\\ldots,x_k)} $$\n",
    "\n",
    "For a given input $(x_1,\\ldots,x_k)$, $P(x_1,\\ldots,x_k)$ is constant. Hence, we can say that:\n",
    "$$ P(y\\ \\mid\\ x_1,\\ldots,x_k) \\propto P(y) \\prod_{i=1}^{k}P(x_i\\ \\mid\\ y) $$\n",
    "\n",
    "Thus, the class of a new instance can be predicted as:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{k}P(x_i\\ \\mid\\ y)$$\n",
    "\n",
    "where $P(y)$ is commonly known as the **class prior** and $P(x_i\\ \\mid\\ y)$ is the **feature predictor**.\n",
    "\n",
    "Observe that this is the product of $k+1$ probability values, which can result in very small numbers. When working with real-world data, this often leads to an [arithmetic underflow](https://en.wikipedia.org/wiki/Arithmetic_underflow). We will instead be adding the logarithm of the probabilities:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_y \\underbrace{\\log P(y)}_\\text{log-prior} + \\underbrace{\\sum_{i=1}^{k} \\log P(x_i\\ \\mid\\ y)}_\\text{log-likelihood}$$\n",
    "\n",
    "The rest of the assignment deals with how each of these probability distributions -- $P(y), P(x_1\\ \\mid\\ y), \\ldots, P(x_k\\ \\mid\\ y)$ -- are estimated from data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Predictor\n",
    "\n",
    "Naive Bayes classifiers are popular because we can independently model each feature and mix-and-match model types based on the prior knowledge. For example, we might know (or assume) that $(X_i|y)$ has some distribution, so we can directly use the probability density or mass function of the distribution to model $(X_i|y)$.\n",
    "\n",
    "In this assignment, you will be using two classes of likelihood models:\n",
    "- Gaussian models, for continuous real-valued features (parameterized by mean $\\mu$ and variance $\\sigma$)\n",
    "- Categorical models, for features in discrete categories (parameterized by $\\mathbf{p} = <p_0,p_1\\ldots>$, one parameter per category)\n",
    "\n",
    "You need to implement a generic predictor class for each type of model. Your class should have the following methods:\n",
    "\n",
    "- `fit()`: Learn parameters for the likelihood model using an appropriate Maximum Likelihood Estimator.\n",
    "- `partial_log_likelihood()`: Use the previously learnt parameters to compute the probability density or mass of a given feature value, and return the natural logarithm of this value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Gaussian Feature Predictor\n",
    "\n",
    "The Gaussian distribution is characterized by two parameters - mean $\\mu$ and standard deviation $\\sigma$:\n",
    "$$ f_Z(z) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp{(-\\frac{(z-\\mu)^2}{2\\sigma^2})} $$\n",
    "\n",
    "Given $n$ samples $z_1, \\ldots, z_n$ from the above distribution, the MLE for mean and standard deviation are:\n",
    "$$ \\hat{\\mu} = \\frac{1}{n} \\sum_{j=1}^{n} z_j $$\n",
    "\n",
    "$$ \\hat{\\sigma} = \\sqrt{\\frac{1}{n} \\sum_{j=1}^{n} (z_j-\\hat{\\mu})^2} $$\n",
    "\n",
    "`scipy.stats.norm` may be helpful, as may `pandas.DataFrame.var`. If you use the latter, remember to correctly set the `ddof=0`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_pred_test(gaussian_predictor):\n",
    "    g = gaussian_predictor(2)\n",
    "    \n",
    "    np.random.seed(0xDEADBEEF)\n",
    "    rnd = np.random.normal(loc=0.0, scale=1.0, size=(1000,))\n",
    "\n",
    "    data = pd.Series(np.concatenate([rnd, 100-rnd]))\n",
    "    labels = pd.Series(np.array([0]*1000 + [1]*1000))\n",
    "\n",
    "    g.fit(data, labels)\n",
    "\n",
    "    test.equal(str(type(g.mu)), \"<class 'numpy.ndarray'>\")\n",
    "    test.true(np.allclose(g.mu, [0, 100], atol=.1))\n",
    "    test.equal(str(type(g.sigma)), \"<class 'numpy.ndarray'>\")\n",
    "    test.true(np.allclose(g.sigma, [1, 1], atol=.1))\n",
    "\n",
    "    test.equal(tuple(g.partial_log_likelihood([0., 50., 100.]).shape), (2, 3))\n",
    "    # If the equality is not exact, you may need to change the test to ensure the absolute difference is no more than 1e-4\n",
    "    test.true(np.allclose(g.partial_log_likelihood([0., 50., 100.]), [[-0.9234573135702573, -1242.233086628376, -4963.217354198167], [-4963.217354198166, -1242.2330866283753, -0.9234573135702564]], rtol=0, atol=1e-4))\n",
    "\n",
    "    df = load_data()\n",
    "    x, y = df['age'].copy(), df[\"label\"].copy()\n",
    "    g = gaussian_predictor(2)\n",
    "    g.fit(x[::6], y[::6])\n",
    "    test.true(np.allclose(g.mu,    [36.4382798, 43.9      ], atol=1e-5))\n",
    "    test.true(np.allclose(g.sigma, [13.36477864, 10.53337101], atol=1e-5))\n",
    "    test.true(np.allclose(g.partial_log_likelihood(x[:10]),\n",
    "            [[-3.52993135, -4.02640576, -3.51838868, -4.27937766, -3.71088289, -3.51244458, -3.95327891, -4.18945513, -3.59434973, -3.59815072],\n",
    "            [-3.38168698, -3.44117221, -3.43035671, -3.64666665, -4.41276436, -3.48803936, -3.39069989, -3.56915559, -4.02340647, -3.28975525]], atol=1e-5))\n",
    "\n",
    "    \n",
    "class GaussianPredictor:\n",
    "    \"\"\" Feature predictor for a normally distributed real-valued, continuous feature.\n",
    "\n",
    "        attr:\n",
    "            k : int -- number of classes\n",
    "            mu : np.ndarray[k] -- vector containing per class mean of the feature\n",
    "            sigma : np.ndarray[k] -- vector containing per class std. deviation of the feature\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        \"\"\" constructor\n",
    "\n",
    "        args : k -- number of classes\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"update predictor statistics (mu, sigma) for Gaussian distribution\n",
    "\n",
    "        args:\n",
    "            x : pd.Series -- feature values\n",
    "            y : np.Series -- class labels\n",
    "            \n",
    "        return : GaussianPredictor -- return self for convenience\n",
    "        \"\"\"\n",
    "        return self\n",
    "            \n",
    "    def partial_log_likelihood(self, x):\n",
    "        \"\"\" log likelihood of feature values x according to each class\n",
    "\n",
    "        args:\n",
    "            x : pd.Series -- feature values\n",
    "\n",
    "        return: np.ndarray[self.k, len(x)] : log likelihood for this feature for each class\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "@test\n",
    "def gaussian_pred(k):\n",
    "    return GaussianPredictor(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Categorical Feature Predictor\n",
    "\n",
    "The categorical distribution with $l$ possible values $\\{\\text A, \\text B, \\text C, \\ldots\\}$ is characterized by the probability distribution $\\mathbf p$ over these values. ($\\mathbf{p} = (p_0,\\dots,p_{l-1})$ where $\\sum\\mathbf p = 1$.)\n",
    "\n",
    "If $C$ is categorically distributed, the probability of observing a particular value $z$ is:\n",
    "\n",
    "$$ \\Pr(C=z; \\mathbf{p}) = \\begin{cases}\n",
    "    p_0 & \\text{ if } z=0\n",
    "\\\\  p_1 & \\text{ if } z=1\n",
    "\\\\  \\vdots\n",
    "\\\\  p_{l-1} & \\text{ if } z=(l-1)\n",
    "\\end{cases}$$\n",
    "\n",
    "Given $n$ samples $z_1, \\ldots, z_n$ from $C$, the smoothed Maximum Likelihood Estimator for $\\mathbf p$ is:\n",
    "$$ \\hat{p_t} = \\frac{\\sum_{j=1}^{n} [z_j=t] + \\alpha}{n + l\\alpha} $$\n",
    "\n",
    "The term in the numerator $\\sum_{j=1}^{n} [z_j=t]$ is the number of times the value $t$ occurred in the sample. The smoothing is done over all possible values that may be generated by $C$. (This avoids the zero-count problem, similar to the smooting done in $n$-gram language models.)\n",
    "\n",
    "In this problem, you need to write a predictor that learns a *different* categorical distribution $C_i$ for each of $k$ possible classes.\n",
    "\n",
    "#### Specification\n",
    "\n",
    "  1. You should maintain a dictionary from each possible input token (i.e. each value) to an array of length $k$ that contains $(\\Pr(C_0=z), \\Pr(C_1=z), ..., \\Pr(C_{k-1}=z))$.\n",
    "  2. For the purpose of smoothing, you should assume that all distributions can produce each value. That is, the set of possible values is the same for all $C_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_pred_test(categorical_pred):\n",
    "    # Test One:\n",
    "    p = categorical_pred(3)\n",
    "    \n",
    "    data = pd.Series([\"A\"]*99 + [\"B\"]*99 + [\"C\"]*99)\n",
    "    labels = pd.Series([0]*99 + [1]*99 + [2]*99)\n",
    "    p.fit(data, labels)\n",
    "    \n",
    "    test.true(np.allclose(p.p['A'], [0.98039216, 0.00980392, 0.00980392], atol=1e-6))\n",
    "\n",
    "    pll = p.partial_log_likelihood([\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"])\n",
    "    test.equal(tuple(pll.shape), (3, 6))\n",
    "    n = np.log(1/102)\n",
    "    p = np.log(100/102)\n",
    "    \n",
    "    test.true(np.allclose(pll, [[p, n, n, p, n, n], [n, p, n, n, p, n], [n, n, p, n, n, p]]))\n",
    "    \n",
    "    # Test Two:\n",
    "    \n",
    "    p = categorical_pred(2)\n",
    "    \n",
    "    data = pd.Series([\"A\"]*50 + [\"B\"]*50 + [\"C\"]*50)\n",
    "    labels = pd.Series([0]*75 + [1]*75)\n",
    "    p.fit(data, labels)\n",
    "    \n",
    "    test.true(np.allclose(p.p['A'], [0.65384614, 0.01282051], atol=1e-6))\n",
    "    test.true(np.allclose(p.p['B'], [0.33333334, 0.33333334], atol=1e-6))\n",
    "    test.true(np.allclose(p.p['C'], [0.01282051, 0.65384614], atol=1e-6))\n",
    "\n",
    "    pll = p.partial_log_likelihood([\"A\", \"B\", \"C\"])\n",
    "    test.equal(tuple(pll.shape), (2, 3))\n",
    "    n = np.log(1/78)\n",
    "    m = np.log(51/78)\n",
    "    l = np.log(26/78)\n",
    "\n",
    "    test.true(np.allclose(pll, [[m, l, n], [n, l, m]], atol=1e-6))\n",
    "    # If test two fails but test one passes, check the smoothing term in the denominator!\n",
    "\n",
    "    df = load_data()\n",
    "    x, y = df['native_country'].copy(), df[\"label\"].copy()    \n",
    "    p = categorical_pred(2)\n",
    "    p.fit(x[::2], y[::2])\n",
    "    \n",
    "    test.true(np.allclose(p.p[\"United-States\"], [0.8994015 , 0.92182153]))\n",
    "    test.true(np.allclose(p.p[\"Cuba\"         ], [0.00352051, 0.0028955 ]))\n",
    "    test.true(np.allclose(p.p[\"Jamaica\"      ], [0.00422461, 0.00157936]))\n",
    "    test.true(np.allclose(p.p[\"England\"      ], [0.00237634, 0.00342195]))\n",
    "\n",
    "    test.true(np.allclose(p.partial_log_likelihood(x[:10]), \n",
    "         [[-0.10602575540542603, -0.10602575540542603, -0.10602575540542603, -0.10602575540542603, -5.649150371551514, -0.10602575540542603, -5.4668288230896, -0.10602575540542603, -0.10602575540542603, -0.10602575540542603],\n",
    "          [-0.08140363544225693, -0.08140363544225693, -0.08140363544225693, -0.08140363544225693, -5.844597816467285, -0.08140363544225693, -6.450733661651611, -0.08140363544225693, -0.08140363544225693, -0.08140363544225693]]))\n",
    "\n",
    "\n",
    "class CategoricalPredictor:\n",
    "    \"\"\" Feature predictor for a categorical feature.\n",
    "\n",
    "        attr: \n",
    "            k : int -- number of classes\n",
    "            p : Dict[feature_value, np.ndarray[k]] -- dictionary of vectors containing per-class probability of a feature value;\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        \"\"\" constructor\n",
    "\n",
    "        args : k -- number of classes\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, x, y, alpha=1.):\n",
    "        \"\"\" initializes the predictor statistics (p) for Categorical distribution\n",
    "        \n",
    "        args:\n",
    "            x : pd.Series -- feature values\n",
    "            y : pd.Series -- class labels\n",
    "        \n",
    "        kwargs:\n",
    "            alpha : float -- smoothing factor\n",
    "\n",
    "        return : CategoricalPredictor -- returns self for convenience\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def partial_log_likelihood(self, x):\n",
    "        \"\"\" log likelihood of feature values x according to each class\n",
    "\n",
    "        args:\n",
    "            x : pd.Series -- vector of feature values\n",
    "\n",
    "        return : np.ndarray[self.k, len(x)] -- matrix of log likelihood for this feature\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "@test\n",
    "def categorical_pred(k):\n",
    "    return CategoricalPredictor(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Putting things together\n",
    "\n",
    "It's time to put all the feature predictors together and do something useful! You will implement a class that puts these classifiers to good use:\n",
    "\n",
    "- `__init__()`: Compute the log prior for each class and initialize the feature predictors (based on feature type). The smoothed prior for class $t$ is given by\n",
    "$$ \\text{prior}(t) = \\frac{n_t + \\alpha}{n + k\\alpha} $$\n",
    "where $n_t = \\sum_{j=1}^{n} [y_j=t]$, (i.e., the number of times the label $t$ occurred in the sample), $n$ is the number fo entries in the sample, and $k$ is the number of label values. \n",
    "- `log_likelihood()`: Compute the sum of the log prior and partial log likelihoods for all features. Use it to predict the final class label.\n",
    "- `predict()`: Use the output of log_likelihood to predict a class label; break ties by predicting the class with lower id.\n",
    "\n",
    "**Note:** Your implementation should not assume the data will always be the same as the census data. We may pass any dataset to your class. You can assume that:\n",
    "\n",
    "1. the input will contain a `label` column of type `int64` with values $0,\\ldots,k-1$ for some $k$\n",
    "2. all other columns will be either of type `object` (for categorical data) or `int64` (for integer data)\n",
    "3. if you encounter a column of an invalid type, throw an exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_test(naive_bayes):\n",
    "    df = load_data()\n",
    "    cl = naive_bayes(df)\n",
    "    test.equal(cl.log_prior.tolist(), [-0.28626858222129903, -1.3905468592226538])\n",
    "\n",
    "    test.true(isinstance(cl.predictor['age'], GaussianPredictor) and\n",
    "        isinstance(cl.predictor['work_class'], CategoricalPredictor) and\n",
    "        isinstance(cl.predictor['final_weight'], GaussianPredictor) and\n",
    "        isinstance(cl.predictor['education'], CategoricalPredictor) and\n",
    "        isinstance(cl.predictor['education_num'], GaussianPredictor) and\n",
    "        isinstance(cl.predictor['marital_status'], CategoricalPredictor) and\n",
    "        isinstance(cl.predictor['occupation'], CategoricalPredictor) and\n",
    "        isinstance(cl.predictor['relationship'], CategoricalPredictor) and\n",
    "        isinstance(cl.predictor['race'], CategoricalPredictor) and\n",
    "        isinstance(cl.predictor['sex'], CategoricalPredictor) and\n",
    "        isinstance(cl.predictor['capital_gain'], GaussianPredictor) and\n",
    "        isinstance(cl.predictor['capital_loss'], GaussianPredictor) and\n",
    "        isinstance(cl.predictor['hours_per_week'], GaussianPredictor) and\n",
    "        isinstance(cl.predictor['native_country'], CategoricalPredictor))    \n",
    "\n",
    "    ll = cl.log_likelihood(df.drop(\"label\", axis=\"columns\"))\n",
    "    test.equal(tuple(ll.shape), (2, 30162))\n",
    "    test.true(np.allclose(ll[:,:2].tolist(), [[-49.84977999441486, -50.38520793711001], [-53.407383777033196, -51.30832341372758]]))\n",
    "\n",
    "    lp = cl.predict(df.drop(\"label\", axis=\"columns\"))\n",
    "    test.equal(tuple(lp.shape), (30162,))\n",
    "    test.equal(sum(lp), 5407)\n",
    "    test.equal(lp[:10].tolist(), [0]*8 + [1]*2)\n",
    "    \n",
    "    # Make sure you can handle a non-contiguous index:\n",
    "    cl = naive_bayes(df[1::3])\n",
    "    ll = cl.log_likelihood(df.drop(\"label\", axis=\"columns\"))\n",
    "    test.equal(tuple(ll.shape), (2, 30162))\n",
    "\n",
    "    # Make sure you can handle a non-contiguous index, pt. 2:\n",
    "    tdf = df.drop([\"age\", \"sex\", \"work_class\"], axis=\"columns\")\n",
    "    cl = naive_bayes(tdf[1::2])\n",
    "    ll = cl.log_likelihood(df.drop(\"label\", axis=\"columns\"))\n",
    "    test.equal(tuple(ll.shape), (2, 30162))\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    \"\"\" Naive Bayes classifier for a mixture of continuous and categorical attributes.\n",
    "        We use GaussianPredictor for continuous attributes and CategoricalPredictor for categorical ones.\n",
    "        \n",
    "        attr:\n",
    "            predictor : Dict[column_name,model] -- model for each column\n",
    "            log_prior : np.ndarray -- the (log) prior probability of each class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, alpha=1.):\n",
    "        \"\"\"initialize predictors for each feature and compute class prior\n",
    "        \n",
    "        args:\n",
    "            df : pd.DataFrame -- processed dataframe, without any missing values.\n",
    "        \n",
    "        kwargs:\n",
    "            alpha : float -- smoothing factor for prior probability\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def log_likelihood(self, x):\n",
    "        \"\"\"log_likelihood for input instances from log_prior and partial_log_likelihood of feature predictors\n",
    "\n",
    "        args:\n",
    "            x : pd.DataFrame -- processed dataframe (ignore label if present)\n",
    "\n",
    "        returns : np.ndarray[num_classes, len(x)] -- array of log-likelihood\n",
    "        \"\"\"\n",
    "        pass            \n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"predicts label for input instances, breaks ties in favor of the class with lower id.\n",
    "\n",
    "        args:\n",
    "            x : pd.DataFrame -- processed dataframe (ignore label if present)\n",
    "\n",
    "        returns : np.ndarray[len(x)] -- vector of class labels\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "@test\n",
    "def naive_bayes(*args, **kwargs):\n",
    "    return NaiveBayesClassifier(*args, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
